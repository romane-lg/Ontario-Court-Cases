{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2537400d-aa93-43a3-97d0-ced9a937cab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ONTARIO COURT OF JUSTICE - COMPLEXITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "‚úì Case texts directory: /Users/tszyan/Downloads/court_case_texts_cleaned/\n",
      "‚úì Output directory: /Users/tszyan/Downloads/outputs/\n",
      "\n",
      "================================================================================\n",
      "STEP 1: PROCESSING CASE FILES\n",
      "================================================================================\n",
      "\n",
      "[1.1] Finding case files...\n",
      "  ‚úì Found 510 case files\n",
      "\n",
      "[1.2] Analyzing cases...\n",
      "    Processed 100/510 files...\n",
      "    Processed 200/510 files...\n",
      "    Processed 300/510 files...\n",
      "    Processed 400/510 files...\n",
      "    Processed 500/510 files...\n",
      "\n",
      "  ‚úì Successfully analyzed 386 cases\n",
      "  ‚úì Skipped 124 invalid files\n",
      "\n",
      "================================================================================\n",
      "STEP 2: DESCRIPTIVE STATISTICS\n",
      "================================================================================\n",
      "\n",
      "üìä SAMPLE SIZE:\n",
      "  Total cases analyzed: 386\n",
      "\n",
      "üìù DOCUMENT STATISTICS:\n",
      "  Mean word count: 3086\n",
      "  Median word count: 2538\n",
      "  Range: 239 - 38801 words\n",
      "  Mean sentence count: 368.8\n",
      "  Mean sentence length: 8.7 words\n",
      "\n",
      "üí≠ SENTIMENT DISTRIBUTION:\n",
      "  Positive: 253 cases ( 65.5%)\n",
      "  Neutral :  94 cases ( 24.4%)\n",
      "  Negative:  39 cases ( 10.1%)\n",
      "  Mean sentiment score: 0.0090\n",
      "\n",
      "üî¨ COMPLEXITY METRICS:\n",
      "  Mean citations: 47.5\n",
      "  Median citations: 34\n",
      "  Mean conversion words: 16.9\n",
      "  Mean complexity score: 0.0217\n",
      "  Mean lexical diversity: 0.3770\n",
      "\n",
      "üìä COMPLEXITY DISTRIBUTION:\n",
      "  Low       : 153 cases ( 39.6%)\n",
      "  Medium    :  97 cases ( 25.1%)\n",
      "  High      :  61 cases ( 15.8%)\n",
      "  Very Low  :  51 cases ( 13.2%)\n",
      "  Very High :  24 cases (  6.2%)\n",
      "\n",
      "================================================================================\n",
      "STEP 3: JUDGE-LEVEL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "‚öñÔ∏è  TOP 10 JUDGES BY COMPLEXITY:\n",
      "  Judge                     Cases  Complexity  Citations  Conversions\n",
      "  ----------------------------------------------------------------------\n",
      "  SPEYER HEARD ON DECEMBER     1    0.0627      91.0       10.0\n",
      "  C.A. BRANNAGAN               4    0.0484     148.2       17.0\n",
      "  CIDALIA FARIA                3    0.0481      51.3       13.3\n",
      "  CITATION                     1    0.0478     108.0       24.0\n",
      "  DAVID PORTER HEARD ON JULY    1    0.0429      69.0        6.0\n",
      "  JENNIFER CAMPITELLI          2    0.0419      73.5       24.5\n",
      "  ANGELA L. MCLEOD             2    0.0409     118.5       18.0\n",
      "  D. ROSE                      1    0.0406      97.0       20.0\n",
      "  PETER N. FRASER              1    0.0400      94.0       13.0\n",
      "  JOSEPH CALLAGHAN             3    0.0391      86.3       14.7\n",
      "\n",
      "================================================================================\n",
      "STEP 4: MOST & LEAST COMPLEX CASES\n",
      "================================================================================\n",
      "\n",
      "üîù TOP 10 MOST COMPLEX CASES:\n",
      "  Case ID      Title                                    Score    Citations  Conversions\n",
      "  -----------------------------------------------------------------------------------------------\n",
      "  case_473     R. v. D.B.                               0.0914     67        10\n",
      "  case_78      R. v. JERLO                              0.0833    165        24\n",
      "  case_46      R. v. LAM                                0.0666    153        18\n",
      "  case_176     R. v. RUSSELL                            0.0649    141         9\n",
      "  case_486     R. v. B.S.                               0.0627     91        10\n",
      "  case_362     R. v. GAUTHIER                           0.0595    111        10\n",
      "  case_28      R. v. ANDERSON                           0.0586    199        23\n",
      "  case_444     R. v. JOHNSON                            0.0563     84        11\n",
      "  case_0       R. v. M.T.                               0.0562    159        20\n",
      "  case_30      R. v. BEITES                             0.0559     33         4\n",
      "\n",
      "üîª TOP 10 LEAST COMPLEX CASES:\n",
      "  Case ID      Title                                    Score    Citations  Conversions\n",
      "  -----------------------------------------------------------------------------------------------\n",
      "  case_440     R. v. CHAND                              0.0022      1         0\n",
      "  case_451     R. v. AY                                 0.0023      1         0\n",
      "  case_480     R. v. CRUZ                               0.0023      1         2\n",
      "  case_48      R. v. MURRAY                             0.0028      6       102\n",
      "  case_57      R. v. MOQUIN                             0.0028      2         4\n",
      "  case_160     R. v. REZAEI                             0.0029      3         3\n",
      "  case_359     R. v. EDGAR                              0.0032      1         3\n",
      "  case_165     R. v. SAINI                              0.0040      2         3\n",
      "  case_216     R. v. LUNDI                              0.0042      5        19\n",
      "  case_11      R. v. I-B.                               0.0044      2         8\n",
      "\n",
      "================================================================================\n",
      "STEP 5: CORRELATION ANALYSIS (TEXT FEATURES)\n",
      "================================================================================\n",
      "\n",
      "üìà CORRELATIONS BETWEEN TEXT FEATURES:\n",
      "  Variable 1           Variable 2                  r   p-value\n",
      "  -----------------------------------------------------------------\n",
      "  Citations            Word Count              0.451    0.0000 **\n",
      "  Conversions          Word Count              0.818    0.0000 **\n",
      "  Complexity           Word Count             -0.072    0.1560 \n",
      "  Complexity           Sentiment               0.005    0.9194 \n",
      "  Citations            Conversions             0.567    0.0000 **\n",
      "  Lexical Diversity    Complexity              0.005    0.9228 \n",
      "\n",
      "================================================================================\n",
      "STEP 6: SAVING RESULTS\n",
      "================================================================================\n",
      "  ‚úì Saved: /Users/tszyan/Downloads/outputs/complexity_analysis.csv\n",
      "  ‚úì Saved: /Users/tszyan/Downloads/outputs/judge_complexity_stats.csv\n",
      "  ‚úì Saved: /Users/tszyan/Downloads/outputs/complexity_correlations.csv\n",
      "  ‚úì Saved: /Users/tszyan/Downloads/outputs/complexity_summary.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY:\n",
      "  Cases analyzed: 386\n",
      "  Mean complexity: 0.0217\n",
      "  Most complex: 0.0914\n",
      "  Least complex: 0.0022\n",
      "\n",
      "üìÅ OUTPUT FILES:\n",
      "  1. /Users/tszyan/Downloads/outputs/complexity_analysis.csv\n",
      "     ‚Üí All 386 cases with complexity metrics\n",
      "  2. /Users/tszyan/Downloads/outputs/judge_complexity_stats.csv\n",
      "     ‚Üí Judge-level statistics\n",
      "  3. /Users/tszyan/Downloads/outputs/complexity_correlations.csv\n",
      "     ‚Üí Correlations between text features\n",
      "  4. /Users/tszyan/Downloads/outputs/complexity_summary.csv\n",
      "     ‚Üí Summary statistics table\n",
      "\n",
      "üéØ KEY FINDINGS:\n",
      "  ‚Ä¢ Complexity range: 0.0022 to 0.0914\n",
      "  ‚Ä¢ Most cases are: Low complexity\n",
      "  ‚Ä¢ Sentiment distribution: Positive cases dominate\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION \n",
    "# ============================================================================\n",
    "\n",
    "CASE_TEXTS_DIR = '/Users/tszyan/Downloads/court_case_texts_cleaned/'  \n",
    "OUTPUT_DIR = '/Users/tszyan/Downloads/outputs/'  \n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ONTARIO COURT OF JUSTICE - COMPLEXITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not os.path.exists(CASE_TEXTS_DIR):\n",
    "    print(f\"\\n‚ùå ERROR: Case texts directory not found!\")\n",
    "    print(f\"   Looking for: {CASE_TEXTS_DIR}\")\n",
    "    print(f\"\\n   Please extract court_case_texts_cleaned.zip and update CASE_TEXTS_DIR\")\n",
    "    exit(1)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úì Case texts directory: {CASE_TEXTS_DIR}\")\n",
    "print(f\"‚úì Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LEXICONS AND WORD LISTS\n",
    "# ============================================================================\n",
    "\n",
    "# Positive legal terms with weights\n",
    "POSITIVE_LEXICON = {\n",
    "    'allowed': 3, 'granted': 3, 'admissible': 4, 'proper': 2, 'reasonable': 2,\n",
    "    'appropriate': 2, 'valid': 3, 'legitimate': 3, 'complies': 2, 'satisfied': 2,\n",
    "    'accepted': 2, 'approved': 3, 'successful': 3, 'favorable': 3, 'supported': 2,\n",
    "    'credible': 3, 'reliable': 2, 'clear': 1, 'accurate': 2, 'adequate': 2,\n",
    "    'justified': 3, 'lawful': 3, 'sound': 2, 'consistent': 2, 'persuasive': 2,\n",
    "}\n",
    "\n",
    "# Negative legal terms with weights\n",
    "NEGATIVE_LEXICON = {\n",
    "    'dismissed': -4, 'denied': -4, 'rejected': -4, 'inadmissible': -4,\n",
    "    'prejudice': -3, 'improper': -3, 'violation': -4, 'breach': -4,\n",
    "    'failed': -3, 'contravenes': -4, 'insufficient': -3, 'flawed': -3,\n",
    "    'unreliable': -3, 'questionable': -2, 'doubtful': -2, 'inconsistent': -2,\n",
    "    'unreasonable': -3, 'inadequate': -2, 'deficient': -2, 'unacceptable': -3,\n",
    "}\n",
    "\n",
    "# Conversion/transition words (argumentative complexity markers)\n",
    "CONVERSION_WORDS = {\n",
    "    'however', 'whereas', 'notwithstanding', 'nevertheless', 'nonetheless',\n",
    "    'although', 'though', 'despite', 'conversely', 'contrary',\n",
    "    'moreover', 'furthermore', 'additionally', 'accordingly', 'consequently',\n",
    "    'therefore', 'thus', 'hence', 'subsequently', 'alternatively',\n",
    "    'similarly', 'likewise', 'whereas', 'whilst',\n",
    "}\n",
    "\n",
    "# Stopwords\n",
    "ENGLISH_STOPWORDS = {\n",
    "    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "    'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',\n",
    "    'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',\n",
    "    'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that',\n",
    "    'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what',\n",
    "    'which', 'who', 'when', 'where', 'why', 'how', 'all', 'each', 'every',\n",
    "}\n",
    "\n",
    "LEGAL_STOPWORDS = {\n",
    "    'court', 'section', 'subsection', 'shall', 'may', 'pursuant', 'case',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text to lowercase words\"\"\"\n",
    "    return re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "\n",
    "def calculate_sentiment(tokens):\n",
    "    \"\"\"Calculate sentiment score using lexicons\"\"\"\n",
    "    pos_score = sum(POSITIVE_LEXICON.get(t, 0) for t in tokens)\n",
    "    neg_score = sum(NEGATIVE_LEXICON.get(t, 0) for t in tokens)\n",
    "    total_score = (pos_score + neg_score) / len(tokens) if tokens else 0\n",
    "    \n",
    "    # Classify sentiment\n",
    "    if total_score > 0.005:\n",
    "        sentiment_class = 'Positive'\n",
    "    elif total_score < -0.005:\n",
    "        sentiment_class = 'Negative'\n",
    "    else:\n",
    "        sentiment_class = 'Neutral'\n",
    "    \n",
    "    return total_score, pos_score, neg_score, sentiment_class\n",
    "\n",
    "def count_citations(text):\n",
    "    \"\"\"Count legal citations using regex patterns\"\"\"\n",
    "    patterns = [\n",
    "        r'\\bR\\.\\s+v\\.\\s+[A-Z]',      # R. v. Someone\n",
    "        r'\\bs\\.\\s*\\d+',               # s. 123\n",
    "        r'\\bss\\.\\s*\\d+',              # ss. 123\n",
    "        r'\\bsection\\s+\\d+',           # section 123\n",
    "        r'\\d+\\(\\d+\\)',                # 123(4) - subsections\n",
    "        r'Criminal\\s+Code',           # Criminal Code\n",
    "        r'\\[\\d{4}\\]',                 # [2024] citations\n",
    "        r'\\d+\\s+S\\.C\\.R\\.',           # Supreme Court Reports\n",
    "        r'para\\.\\s*\\d+',              # para. 15\n",
    "    ]\n",
    "    return sum(len(re.findall(p, text, re.IGNORECASE)) for p in patterns)\n",
    "\n",
    "def count_conversion_words(tokens):\n",
    "    \"\"\"Count conversion/transition words\"\"\"\n",
    "    return sum(1 for t in tokens if t in CONVERSION_WORDS)\n",
    "\n",
    "def extract_case_title(text):\n",
    "    \"\"\"Extract case title from document\"\"\"\n",
    "    # Try uppercase format first (original)\n",
    "    match = re.search(r'R\\.\\s+v\\.\\s+([A-Z][\\w.-]+)', text)\n",
    "    if match:\n",
    "        return f\"R. v. {match.group(1)}\"\n",
    "    \n",
    "    # Try lowercase format (cleaned files)\n",
    "    match = re.search(r'r\\.\\s+v\\.\\s+([a-z][\\w.-]+)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        name = match.group(1).upper()  # Convert to uppercase\n",
    "        return f\"R. v. {name}\"\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_judge(text):\n",
    "    \"\"\"Extract judge name from document\"\"\"\n",
    "    # Try uppercase format patterns\n",
    "    patterns = [\n",
    "        r'Justice\\s+([A-Z][\\w.\\s]+?)(?:\\n|Heard)',\n",
    "        r'J\\.\\s+([A-Z][\\w.\\s]+?)(?:\\n|Heard)',\n",
    "        r'([A-Z][\\w.\\s]+?)\\s+J\\.',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            name = match.group(1).strip()\n",
    "            if len(name) > 3 and len(name) < 30:\n",
    "                return name\n",
    "    \n",
    "    # Try lowercase format (cleaned files)\n",
    "    # Pattern: \"justice [name]\" followed by newline\n",
    "    match = re.search(r'justice\\s+([a-z][a-z.\\s]+?)(?:\\n|heard)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        name = match.group(1).strip()\n",
    "        # Convert to title case\n",
    "        name = ' '.join(word.upper() for word in name.split())\n",
    "        if len(name) > 3 and len(name) < 30:\n",
    "            return name\n",
    "    \n",
    "    # Try pattern: \"before\\njustice [name]\"\n",
    "    match = re.search(r'before\\s+justice\\s+([a-z][a-z.\\s]+)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        name = match.group(1).strip()\n",
    "        name = ' '.join(word.upper() for word in name.split())\n",
    "        if len(name) > 3 and len(name) < 30:\n",
    "            return name\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "def is_valid_case(text):\n",
    "    \"\"\"Check if document is valid\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Check for CAPTCHA\n",
    "    if 'captcha' in text_lower or 'verification' in text_lower:\n",
    "        return False\n",
    "    \n",
    "    # Check minimum length (lowered threshold for cleaned files)\n",
    "    if len(text) < 200:  # Changed from 500 to 200\n",
    "        return False\n",
    "    \n",
    "    # Check for court-related keywords\n",
    "    court_keywords = ['court', 'judge', 'justice', 'decision', 'ruling', 'judgment', 'criminal code']\n",
    "    if not any(keyword in text_lower for keyword in court_keywords):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def classify_complexity(score):\n",
    "    \"\"\"Classify complexity level\"\"\"\n",
    "    if score >= 0.040:\n",
    "        return 'Very High'\n",
    "    elif score >= 0.030:\n",
    "        return 'High'\n",
    "    elif score >= 0.020:\n",
    "        return 'Medium'\n",
    "    elif score >= 0.010:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Very Low'\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: PROCESSING CASE FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[1.1] Finding case files...\")\n",
    "case_files = sorted(glob.glob(f'{CASE_TEXTS_DIR}case_*.txt'))\n",
    "\n",
    "if len(case_files) == 0:\n",
    "    print(f\"‚ùå ERROR: No case files found in {CASE_TEXTS_DIR}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"  ‚úì Found {len(case_files)} case files\")\n",
    "\n",
    "print(\"\\n[1.2] Analyzing cases...\")\n",
    "results = []\n",
    "all_stopwords = ENGLISH_STOPWORDS.union(LEGAL_STOPWORDS)\n",
    "skipped = 0\n",
    "\n",
    "for i, filepath in enumerate(case_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"    Processed {i+1}/{len(case_files)} files...\")\n",
    "    \n",
    "    try:\n",
    "        case_id = os.path.basename(filepath).replace('.txt', '')\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        if not is_valid_case(text):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Extract metadata\n",
    "        case_title = extract_case_title(text)\n",
    "        judge = extract_judge(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        all_tokens = tokenize(text)\n",
    "        tokens_filtered = [t for t in all_tokens if t not in all_stopwords and len(t) > 2]\n",
    "        \n",
    "        # Calculate sentiment\n",
    "        sentiment, pos_score, neg_score, sentiment_class = calculate_sentiment(tokens_filtered)\n",
    "        \n",
    "        # Calculate complexity metrics\n",
    "        citations = count_citations(text)\n",
    "        conversions = count_conversion_words(all_tokens)\n",
    "        \n",
    "        # Overall complexity score\n",
    "        complexity = (citations + conversions) / len(tokens_filtered) if tokens_filtered else 0\n",
    "        complexity_class = classify_complexity(complexity)\n",
    "        \n",
    "        # Lexical diversity (vocabulary richness)\n",
    "        lexical_div = len(set(tokens_filtered)) / len(tokens_filtered) if tokens_filtered else 0\n",
    "        \n",
    "        # Sentence analysis\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "        avg_sent_len = len(tokens_filtered) / len(sentences) if sentences else 0\n",
    "        \n",
    "        results.append({\n",
    "            'case_id': case_id,\n",
    "            'case_title': case_title,\n",
    "            'judge': judge,\n",
    "            \n",
    "            # Document metrics\n",
    "            'char_count': len(text),\n",
    "            'word_count': len(tokens_filtered),\n",
    "            'unique_words': len(set(tokens_filtered)),\n",
    "            'sentence_count': len(sentences),\n",
    "            'avg_sentence_length': avg_sent_len,\n",
    "            \n",
    "            # Sentiment metrics\n",
    "            'sentiment_score': sentiment,\n",
    "            'sentiment_class': sentiment_class,\n",
    "            'pos_word_score': pos_score,\n",
    "            'neg_word_score': neg_score,\n",
    "            \n",
    "            # Complexity metrics\n",
    "            'citation_count': citations,\n",
    "            'conversion_word_count': conversions,\n",
    "            'complexity_score': complexity,\n",
    "            'complexity_class': complexity_class,\n",
    "            'lexical_diversity': lexical_div,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"\\n  ‚úì Successfully analyzed {len(df)} cases\")\n",
    "print(f\"  ‚úì Skipped {skipped} invalid files\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: DESCRIPTIVE STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä SAMPLE SIZE:\")\n",
    "print(f\"  Total cases analyzed: {len(df)}\")\n",
    "\n",
    "print(f\"\\nüìù DOCUMENT STATISTICS:\")\n",
    "print(f\"  Mean word count: {df['word_count'].mean():.0f}\")\n",
    "print(f\"  Median word count: {df['word_count'].median():.0f}\")\n",
    "print(f\"  Range: {df['word_count'].min():.0f} - {df['word_count'].max():.0f} words\")\n",
    "print(f\"  Mean sentence count: {df['sentence_count'].mean():.1f}\")\n",
    "print(f\"  Mean sentence length: {df['avg_sentence_length'].mean():.1f} words\")\n",
    "\n",
    "print(f\"\\nüí≠ SENTIMENT DISTRIBUTION:\")\n",
    "sentiment_counts = df['sentiment_class'].value_counts()\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  {sentiment:8s}: {count:3d} cases ({pct:5.1f}%)\")\n",
    "print(f\"  Mean sentiment score: {df['sentiment_score'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nüî¨ COMPLEXITY METRICS:\")\n",
    "print(f\"  Mean citations: {df['citation_count'].mean():.1f}\")\n",
    "print(f\"  Median citations: {df['citation_count'].median():.0f}\")\n",
    "print(f\"  Mean conversion words: {df['conversion_word_count'].mean():.1f}\")\n",
    "print(f\"  Mean complexity score: {df['complexity_score'].mean():.4f}\")\n",
    "print(f\"  Mean lexical diversity: {df['lexical_diversity'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nüìä COMPLEXITY DISTRIBUTION:\")\n",
    "complexity_counts = df['complexity_class'].value_counts()\n",
    "for complexity, count in complexity_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  {complexity:10s}: {count:3d} cases ({pct:5.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: JUDGE-LEVEL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: JUDGE-LEVEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "judge_stats = df.groupby('judge').agg({\n",
    "    'case_id': 'count',\n",
    "    'complexity_score': 'mean',\n",
    "    'citation_count': 'mean',\n",
    "    'conversion_word_count': 'mean',\n",
    "    'sentiment_score': 'mean',\n",
    "    'word_count': 'mean',\n",
    "}).round(4)\n",
    "\n",
    "judge_stats.columns = ['case_count', 'avg_complexity', 'avg_citations', \n",
    "                       'avg_conversions', 'avg_sentiment', 'avg_words']\n",
    "judge_stats = judge_stats.sort_values('avg_complexity', ascending=False)\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  TOP 10 JUDGES BY COMPLEXITY:\")\n",
    "print(f\"  {'Judge':<25} Cases  Complexity  Citations  Conversions\")\n",
    "print(f\"  {'-'*70}\")\n",
    "for judge, row in judge_stats.head(10).iterrows():\n",
    "    print(f\"  {judge:<25} {row['case_count']:4.0f}    {row['avg_complexity']:.4f}     \"\n",
    "          f\"{row['avg_citations']:5.1f}      {row['avg_conversions']:5.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: TOP/BOTTOM CASES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: MOST & LEAST COMPLEX CASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüîù TOP 10 MOST COMPLEX CASES:\")\n",
    "print(f\"  {'Case ID':<12} {'Title':<40} {'Score':<8} Citations  Conversions\")\n",
    "print(f\"  {'-'*95}\")\n",
    "for idx, row in df.nlargest(10, 'complexity_score').iterrows():\n",
    "    title = row['case_title'][:37] + '...' if len(row['case_title']) > 40 else row['case_title']\n",
    "    print(f\"  {row['case_id']:<12} {title:<40} {row['complexity_score']:.4f}   \"\n",
    "          f\"{row['citation_count']:4.0f}      {row['conversion_word_count']:4.0f}\")\n",
    "\n",
    "print(f\"\\nüîª TOP 10 LEAST COMPLEX CASES:\")\n",
    "print(f\"  {'Case ID':<12} {'Title':<40} {'Score':<8} Citations  Conversions\")\n",
    "print(f\"  {'-'*95}\")\n",
    "for idx, row in df.nsmallest(10, 'complexity_score').iterrows():\n",
    "    title = row['case_title'][:37] + '...' if len(row['case_title']) > 40 else row['case_title']\n",
    "    print(f\"  {row['case_id']:<12} {title:<40} {row['complexity_score']:.4f}   \"\n",
    "          f\"{row['citation_count']:4.0f}      {row['conversion_word_count']:4.0f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: CORRELATION ANALYSIS (TEXT FEATURES ONLY)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: CORRELATION ANALYSIS (TEXT FEATURES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "correlations = []\n",
    "variables = [\n",
    "    ('Citations', 'Word Count', 'citation_count', 'word_count'),\n",
    "    ('Conversions', 'Word Count', 'conversion_word_count', 'word_count'),\n",
    "    ('Complexity', 'Word Count', 'complexity_score', 'word_count'),\n",
    "    ('Complexity', 'Sentiment', 'complexity_score', 'sentiment_score'),\n",
    "    ('Citations', 'Conversions', 'citation_count', 'conversion_word_count'),\n",
    "    ('Lexical Diversity', 'Complexity', 'lexical_diversity', 'complexity_score'),\n",
    "]\n",
    "\n",
    "print(f\"\\nüìà CORRELATIONS BETWEEN TEXT FEATURES:\")\n",
    "print(f\"  {'Variable 1':<20} {'Variable 2':<20} {'r':>8}  {'p-value':>8}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "\n",
    "for var1_name, var2_name, var1, var2 in variables:\n",
    "    r, p = stats.pearsonr(df[var1], df[var2])\n",
    "    correlations.append({\n",
    "        'Variable_1': var1_name,\n",
    "        'Variable_2': var2_name,\n",
    "        'Correlation_r': r,\n",
    "        'P_value': p,\n",
    "        'Significant': 'Yes' if p < 0.05 else 'No'\n",
    "    })\n",
    "    sig = '**' if p < 0.05 else ''\n",
    "    print(f\"  {var1_name:<20} {var2_name:<20} {r:8.3f}  {p:8.4f} {sig}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Main results\n",
    "main_output = f'{OUTPUT_DIR}complexity_analysis.csv'\n",
    "df.to_csv(main_output, index=False)\n",
    "print(f\"  ‚úì Saved: {main_output}\")\n",
    "\n",
    "# Judge statistics\n",
    "judge_output = f'{OUTPUT_DIR}judge_complexity_stats.csv'\n",
    "judge_stats.to_csv(judge_output)\n",
    "print(f\"  ‚úì Saved: {judge_output}\")\n",
    "\n",
    "# Correlations\n",
    "corr_output = f'{OUTPUT_DIR}complexity_correlations.csv'\n",
    "pd.DataFrame(correlations).to_csv(corr_output, index=False)\n",
    "print(f\"  ‚úì Saved: {corr_output}\")\n",
    "\n",
    "# Summary statistics\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Cases Analyzed',\n",
    "        '',\n",
    "        'Mean Word Count',\n",
    "        'Mean Sentence Count',\n",
    "        'Mean Sentence Length',\n",
    "        '',\n",
    "        'Mean Citations',\n",
    "        'Mean Conversion Words',\n",
    "        'Mean Complexity Score',\n",
    "        'Mean Lexical Diversity',\n",
    "        '',\n",
    "        'Mean Sentiment Score',\n",
    "        'Positive Cases (%)',\n",
    "        'Neutral Cases (%)',\n",
    "        'Negative Cases (%)',\n",
    "        '',\n",
    "        'Very High Complexity (%)',\n",
    "        'High Complexity (%)',\n",
    "        'Medium Complexity (%)',\n",
    "        'Low Complexity (%)',\n",
    "        'Very Low Complexity (%)',\n",
    "    ],\n",
    "    'Value': [\n",
    "        len(df),\n",
    "        '',\n",
    "        f\"{df['word_count'].mean():.0f}\",\n",
    "        f\"{df['sentence_count'].mean():.1f}\",\n",
    "        f\"{df['avg_sentence_length'].mean():.1f}\",\n",
    "        '',\n",
    "        f\"{df['citation_count'].mean():.1f}\",\n",
    "        f\"{df['conversion_word_count'].mean():.1f}\",\n",
    "        f\"{df['complexity_score'].mean():.4f}\",\n",
    "        f\"{df['lexical_diversity'].mean():.4f}\",\n",
    "        '',\n",
    "        f\"{df['sentiment_score'].mean():.4f}\",\n",
    "        f\"{(sentiment_counts.get('Positive', 0) / len(df) * 100):.1f}\",\n",
    "        f\"{(sentiment_counts.get('Neutral', 0) / len(df) * 100):.1f}\",\n",
    "        f\"{(sentiment_counts.get('Negative', 0) / len(df) * 100):.1f}\",\n",
    "        '',\n",
    "        f\"{(complexity_counts.get('Very High', 0) / len(df) * 100):.1f}\",\n",
    "        f\"{(complexity_counts.get('High', 0) / len(df) * 100):.1f}\",\n",
    "        f\"{(complexity_counts.get('Medium', 0) / len(df) * 100):.1f}\",\n",
    "        f\"{(complexity_counts.get('Low', 0) / len(df) * 100):.1f}\",\n",
    "        f\"{(complexity_counts.get('Very Low', 0) / len(df) * 100):.1f}\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "summary_output = f'{OUTPUT_DIR}complexity_summary.csv'\n",
    "summary_stats.to_csv(summary_output, index=False)\n",
    "print(f\"  ‚úì Saved: {summary_output}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä SUMMARY:\")\n",
    "print(f\"  Cases analyzed: {len(df)}\")\n",
    "print(f\"  Mean complexity: {df['complexity_score'].mean():.4f}\")\n",
    "print(f\"  Most complex: {df['complexity_score'].max():.4f}\")\n",
    "print(f\"  Least complex: {df['complexity_score'].min():.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES:\")\n",
    "print(f\"  1. {main_output}\")\n",
    "print(f\"     ‚Üí All {len(df)} cases with complexity metrics\")\n",
    "print(f\"  2. {judge_output}\")\n",
    "print(f\"     ‚Üí Judge-level statistics\")\n",
    "print(f\"  3. {corr_output}\")\n",
    "print(f\"     ‚Üí Correlations between text features\")\n",
    "print(f\"  4. {summary_output}\")\n",
    "print(f\"     ‚Üí Summary statistics table\")\n",
    "\n",
    "print(f\"\\nüéØ KEY FINDINGS:\")\n",
    "print(f\"  ‚Ä¢ Complexity range: {df['complexity_score'].min():.4f} to {df['complexity_score'].max():.4f}\")\n",
    "print(f\"  ‚Ä¢ Most cases are: {complexity_counts.idxmax()} complexity\")\n",
    "print(f\"  ‚Ä¢ Sentiment distribution: {sentiment_counts.idxmax()} cases dominate\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa74e2a5-1adc-4150-abe7-7d1bca22e826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datasci)",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
